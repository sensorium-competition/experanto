{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1c8676-602f-47bc-8ed0-900157d804f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "from experanto.datasets import ChunkDataset, SimpleChunkedDataset\n",
    "from experanto.utils import LongCycler, MultiEpochsDataLoader\n",
    "from experanto.dataloaders import get_multisession_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecaad720-52d7-4fa4-a5a9-bbb3f48d0808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'global_sampling_rate': 8,\n",
       "  'global_chunk_size': 32,\n",
       "  'modality_config': {'screen': {'sampling_rate': None,\n",
       "    'chunk_size': None,\n",
       "    'valid_condition': {'tier': 'train', 'stim_type': 'stimulus.Clip'},\n",
       "    'offset': 0,\n",
       "    'sample_stride': 4,\n",
       "    'include_blanks': True,\n",
       "    'transforms': {'ToTensor': {'_target_': 'torchvision.transforms.ToTensor'},\n",
       "     'Normalize': {'_target_': 'torchvision.transforms.Normalize',\n",
       "      'mean': 80.0,\n",
       "      'std': 60.0},\n",
       "     'Resize': {'_target_': 'torchvision.transforms.Resize',\n",
       "      'size': [144, 256]},\n",
       "     'CenterCrop': {'_target_': 'torchvision.transforms.CenterCrop',\n",
       "      'size': 144}},\n",
       "    'interpolation': {}},\n",
       "   'responses': {'sampling_rate': None,\n",
       "    'chunk_size': None,\n",
       "    'offset': 0.1,\n",
       "    'transforms': {'standardize': True},\n",
       "    'interpolation': {'interpolation_mode': 'nearest_neighbor'}}}},\n",
       " 'dataloader': {'batch_size': 32,\n",
       "  'shuffle': True,\n",
       "  'num_workers': 1,\n",
       "  'pin_memory': True,\n",
       "  'drop_last': True,\n",
       "  'prefetch_factor': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {'dataset':{\n",
    "        'global_sampling_rate': 8,\n",
    "        'global_chunk_size': 32,\n",
    "        'modality_config':{\n",
    "            'screen': {\n",
    "                    'sampling_rate': None,\n",
    "                    'chunk_size': None,\n",
    "                    'valid_condition': {\n",
    "                        'tier': 'train',\n",
    "                        'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                        'stim_type': 'stimulus.Clip'\n",
    "                    },\n",
    "                    'offset': 0,\n",
    "                    'sample_stride': 4,\n",
    "                    # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                    'include_blanks': True, \n",
    "                    'transforms': {\n",
    "                        'ToTensor': {\n",
    "                            '_target_': 'torchvision.transforms.ToTensor'\n",
    "                        },\n",
    "                        'Normalize': {\n",
    "                            '_target_': 'torchvision.transforms.Normalize',\n",
    "                            'mean': 80.0,\n",
    "                            'std': 60.0\n",
    "                        },\n",
    "                        'Resize': {\n",
    "                            '_target_': 'torchvision.transforms.Resize',\n",
    "                            'size': [144, 256]\n",
    "                        },\n",
    "                        'CenterCrop': {\n",
    "                            '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                            'size': 144\n",
    "                        }\n",
    "                    },\n",
    "                    'interpolation': {}\n",
    "                },\n",
    "                'responses': {\n",
    "                    'sampling_rate': None,\n",
    "                    'chunk_size': None,\n",
    "                    'offset': 0.1,\n",
    "                    'transforms': {\n",
    "                        'standardize': True\n",
    "                    },\n",
    "                    'interpolation': {\n",
    "                        'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'dataloader':{\n",
    "        'batch_size': 32,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 1,\n",
    "        'pin_memory': True,\n",
    "        'drop_last': True,\n",
    "        'prefetch_factor': 1\n",
    "    }\n",
    "}\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6363aef2-5890-4e61-a9ef-0979565a1382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/allen_data/experiment_951980473\n",
      "../data/allen_data/experiment_951980471\n"
     ]
    }
   ],
   "source": [
    "parent_folder = '../data/allen_data'\n",
    "full_paths = [f.path for f in os.scandir(parent_folder) if f.is_dir()]\n",
    "\n",
    "for subfolder in full_paths:\n",
    "    print(subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8745bfc1-16ae-4d16-a7a0-cd21e0a1d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dl = get_multisession_dataloader(full_paths[:2], cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33b20154-782a-4d74-9483-4eed5fd4a820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'951980473': <experanto.utils.MultiEpochsDataLoader at 0x79bd2c119550>,\n",
       " '951980471': <experanto.utils.MultiEpochsDataLoader at 0x79bbf8fd5460>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    }
   ],
   "source": [
    "train_dl.loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343df343-199d-4474-87ef-85e26a6aa889",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
