{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8379a594-a7c8-4af3-9ee5-4e5fbab5a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from experanto.datasets import ChunkDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1953158d-c7ae-4095-9e59-b67d0a19511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '../data/allen_data'\n",
    "sampling_rate = 8  # Timestamps generated by this do not match the real ones, why do this? Is this for output data?\n",
    "chunk_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6c6d219-6a13-49eb-8574-aae1248c797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980471', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'train',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    }\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c6d82b-1628-4a67-85f3-cff95acd8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980473', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'val',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    }\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab29670d-fcb5-4d89-9216-8e82652ba02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 15\n",
    "data_loaders = OrderedDict()\n",
    "m = 'allen_data'\n",
    "\n",
    "data_loaders['train'] = OrderedDict()\n",
    "data_loaders['oracle'] = OrderedDict()\n",
    "data_loaders['train'][m] =  DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "data_loaders['oracle'][m] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc27547e-47be-420f-9aa4-ddd88011bc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/src/sensorium_2023/')\n",
    "import torch\n",
    "from sensorium.datasets.mouse_video_loaders import mouse_video_loader\n",
    "from sensorium.utility.scores import get_correlations\n",
    "from nnfabrik.builder import get_trainer\n",
    "from sensorium.models.make_model import make_video_model\n",
    "from nnfabrik.utility.nn_helpers import set_random_seed\n",
    "seed = 42\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c16c84c2-52cd-402c-8914-aeaf67f0de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "factorised_3D_core_dict = dict(\n",
    "    input_channels=3, # With this we can use both rgb and greyscale,  addapt this for me?\n",
    "    hidden_channels=[32, 64, 128],\n",
    "    spatial_input_kernel=(11,11),\n",
    "    temporal_input_kernel=11,\n",
    "    spatial_hidden_kernel=(5,5),\n",
    "    temporal_hidden_kernel=5,\n",
    "    stride=1,\n",
    "    layers=3,\n",
    "    gamma_input_spatial=10,\n",
    "    gamma_input_temporal=0.01, \n",
    "    bias=True, \n",
    "    hidden_nonlinearities='elu', \n",
    "    x_shift=0, \n",
    "    y_shift=0,\n",
    "    batch_norm=True, \n",
    "    laplace_padding=None,\n",
    "    input_regularizer='LaplaceL2norm',\n",
    "    padding=False,\n",
    "    final_nonlin=True,\n",
    "    momentum=0.7\n",
    ")\n",
    "\n",
    "\n",
    "shifter_dict=None\n",
    "\n",
    "\n",
    "readout_dict = dict(\n",
    "    bias=True,\n",
    "    init_mu_range=0.2,\n",
    "    init_sigma=1.0,\n",
    "    gamma_readout=0.0,\n",
    "    gauss_type='full',\n",
    "    grid_mean_predictor=None,\n",
    "    #grid_mean_predictor={\n",
    "    #    'type': 'cortex',\n",
    "    #    'input_dimensions': 2,\n",
    "    #    'hidden_layers': 1,\n",
    "    #    'hidden_features': 30,\n",
    "    #    'final_tanh': True\n",
    "    #},\n",
    "    share_features=False,\n",
    "    share_grid=False,\n",
    "    shared_match_ids=None,\n",
    "    gamma_grid_dispersion=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a450c309-2883-43e8-8fa6-7ba56a866ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The bias is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/cores/base.py:82: UserWarning: The batch_norm_scale is applied to all layers\n",
      "  warnings.warn(f\"The {attr} is applied to all layers\", UserWarning)\n",
      "/src/neuralpredictors/neuralpredictors/layers/readouts/base.py:74: UserWarning: Use of 'gamma_readout' is deprecated. Use 'feature_reg_weight' instead. If 'feature_reg_weight' is defined, 'gamma_readout' is ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "factorised_3d_model = make_video_model(\n",
    "    data_loaders,\n",
    "    seed,\n",
    "    core_dict=factorised_3D_core_dict,\n",
    "    core_type='3D_factorised',\n",
    "    readout_dict=readout_dict.copy(),\n",
    "    readout_type='gaussian',               \n",
    "    use_gru=False,\n",
    "    gru_dict=None,\n",
    "    use_shifter=False,\n",
    "    shifter_dict=shifter_dict,\n",
    "    shifter_type='MLP',\n",
    "    deeplake_ds=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9c16fc-5bf5-46be-bfee-7c1f1b7ad5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VideoFiringRateEncoder(\n",
       "  (core): Factorized3dCore(\n",
       "    (_input_weight_regularizer): LaplaceL2norm(\n",
       "      (laplace): Laplace()\n",
       "    )\n",
       "    (temporal_regularizer): DepthLaplaceL21d(\n",
       "      (laplace): Laplace1d()\n",
       "    )\n",
       "    (features): Sequential(\n",
       "      (layer0): Sequential(\n",
       "        (conv_spatial): Conv3d(3, 32, kernel_size=(1, 11, 11), stride=(1, 1, 1))\n",
       "        (conv_temporal): Conv3d(32, 32, kernel_size=(11, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(32, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer1): Sequential(\n",
       "        (conv_spatial_1): Conv3d(32, 64, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_1): Conv3d(64, 64, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(64, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (conv_spatial_2): Conv3d(64, 128, kernel_size=(1, 5, 5), stride=(1, 1, 1))\n",
       "        (conv_temporal_2): Conv3d(128, 128, kernel_size=(5, 1, 1), stride=(1, 1, 1))\n",
       "        (norm): BatchNorm3d(128, eps=1e-05, momentum=0.7, affine=True, track_running_stats=True)\n",
       "        (nonlin): ELU(alpha=1.0)\n",
       "      )\n",
       "    )\n",
       "  ) [Factorized3dCore regularizers: gamma_input_spatial = 10|gamma_input_temporal = 0.01]\n",
       "  \n",
       "  (readout): MultipleFullGaussian2d(\n",
       "    (allen_data): full FullGaussian2d (128 x 126 x 126 -> 32) with bias\n",
       "  )\n",
       "  (nonlinearity_fn): ELU(alpha=1.0)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorised_3d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "675e1dcb-014c-428a-8132-2d582feb5999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e68d8d4a-76dc-4b0d-a585-7c14a27fec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_fn = \"sensorium.training.video_training_loop.standard_trainer\"\n",
    "\n",
    "trainer_config = {\n",
    "    'dataloaders': data_loaders,  # Keep this as it is (your data loaders)\n",
    "    'seed' : 111,  # Set seed for reproducibility\n",
    "    'use_wandb' : False,  # Disable WandB\n",
    "    'verbose': True,  # Keep verbosity for logging\n",
    "    'lr_decay_steps': 1,  # One decay step (this will not matter for 1 iteration)\n",
    "    'lr_init': 0.005,  # Keep the initial learning rate the same\n",
    "    'device' : device,  # Keep the device (cpu or cuda) unchanged\n",
    "    'detach_core' : False,  # Set False to allow gradients for the core\n",
    "    'deeplake_ds' : False,  # Set to False as you're not using DeepLake\n",
    "    'checkpoint_save_path': '/tmp/',  # Save checkpoints temporarily or disable saving\n",
    "    'max_iter': 1,  # Set max_iter to 1 for a quick test (1 iteration)\n",
    "    'batch_size': 1,  # Small batch size (1) for fast testing\n",
    "}\n",
    "\n",
    "\n",
    "trainer = get_trainer(trainer_fn=trainer_fn, \n",
    "                 trainer_config=trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939376b2-83a6-47ff-b873-13d4dac8f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_score, trainer_output, state_dict = trainer(factorised_3d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57268626-dfe3-4b4c-8b53-71fa82d6ba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7f0b8-1301-4907-8758-06fe50c47108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
