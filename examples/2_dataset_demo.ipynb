{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b0faf41-e037-4a3b-8bdb-7ae89b4c9743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from experanto.datasets import ChunkDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a40f03fe-6b43-4b9d-8f62-030e145f7b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '../data/allen_data'\n",
    "sampling_rate = 8  \n",
    "chunk_size = 32 # since we also use video data we always use chunks of images to also consider temporal developements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568cd9c1-9a7c-4f3e-99c9-1c8ca0382d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample modality config for a trainingset which includes screen and response interpolation\n",
    "\n",
    "train_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980471', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'train',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    },\n",
    "                    'greyscale': True # add this for greyscale data\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ac97e24-1e34-4df1-8d5c-69fc403bb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ChunkDataset(root_folder=f'{root_folder}/experiment_951980473', global_sampling_rate=sampling_rate,\n",
    "            global_chunk_size=chunk_size,\n",
    "            modality_config = \n",
    "            {'screen': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'valid_condition': {\n",
    "                    'tier': 'val',\n",
    "                    'stim_type': 'stimulus.Frame', #include both images and videos\n",
    "                    'stim_type': 'stimulus.Clip'\n",
    "                },\n",
    "                'offset': 0,\n",
    "                'sample_stride': 4,\n",
    "                # necessary for the allen dataset since there are blanks after every stimuli because else no valid times are found\n",
    "                'include_blanks': True, \n",
    "                'transforms': {\n",
    "                    'ToTensor': {\n",
    "                        '_target_': 'torchvision.transforms.ToTensor'\n",
    "                    },\n",
    "                    'Normalize': {\n",
    "                        '_target_': 'torchvision.transforms.Normalize',\n",
    "                        'mean': 80.0,\n",
    "                        'std': 60.0\n",
    "                    },\n",
    "                    'Resize': {\n",
    "                        '_target_': 'torchvision.transforms.Resize',\n",
    "                        'size': [144, 256]\n",
    "                    },\n",
    "                    'CenterCrop': {\n",
    "                        '_target_': 'torchvision.transforms.CenterCrop',\n",
    "                        'size': 144\n",
    "                    },\n",
    "                    'greyscale': True # add this for greyscale data\n",
    "                },\n",
    "                'interpolation': {}\n",
    "            },\n",
    "            'responses': {\n",
    "                'sampling_rate': None,\n",
    "                'chunk_size': None,\n",
    "                'offset': 0.1,\n",
    "                'transforms': {\n",
    "                    'standardize': True\n",
    "                },\n",
    "                'interpolation': {\n",
    "                    'interpolation_mode': 'nearest_neighbor'\n",
    "                }\n",
    "            },\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7dee680-f6e7-48be-a2ce-fa550f0ada49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['screen', 'responses', 'timestamps'])\n",
      "This is shape torch.Size([1, 32, 144, 144]) for modality screen\n",
      "This is shape torch.Size([32, 12]) for modality responses\n",
      "This is shape torch.Size([32, 1]) for modality timestamps\n"
     ]
    }
   ],
   "source": [
    "# interpolation showcase using the dataset object\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(sample.keys())\n",
    "for key in sample.keys():\n",
    "    print(f'This is shape {sample[key].shape} for modality {key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09491f58-0234-4073-afa5-12b730795a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating dataloaders based on the dataset objects\n",
    "\n",
    "batch_size = 15\n",
    "data_loaders = OrderedDict()\n",
    "\n",
    "data_loaders['train'] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "data_loaders['val'] = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f1d3fb3-bbc7-4102-b74b-dd30d6b1a8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('train',\n",
       "              <torch.utils.data.dataloader.DataLoader at 0x7b01452a48b0>),\n",
       "             ('val',\n",
       "              <torch.utils.data.dataloader.DataLoader at 0x7b01452a4a00>)])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6c3a7b7-3db5-4045-90de-20d1b546c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "Screen Data: torch.Size([15, 1, 32, 144, 144])\n",
      "Responses: torch.Size([15, 32, 12])\n",
      "Timestamps: torch.Size([15, 32, 1])\n"
     ]
    }
   ],
   "source": [
    "# interpolation showcase using the data_loaders\n",
    "for batch_idx, batch_data in enumerate(data_loaders['train']):\n",
    "    # batch_data is a dictionary with keys 'screen', 'responses', and 'timestamps'\n",
    "    screen_data = batch_data['screen']\n",
    "    responses = batch_data['responses']\n",
    "    timestamps = batch_data['timestamps']\n",
    "    \n",
    "    # Print or inspect the batch\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"Screen Data:\", screen_data.shape)\n",
    "    print(\"Responses:\", responses.shape)\n",
    "    print(\"Timestamps:\", timestamps.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbafc9-3f28-4648-a0d6-d9dea74a456c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
